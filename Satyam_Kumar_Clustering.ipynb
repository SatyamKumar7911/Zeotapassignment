
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


customers_df = pd.read_csv(customers.csv)
transactions_df = pd.read_csv(transactions.csv)

# Preprocess the data
customers_df['SignupDate'] = pd.to_datetime(customers_df['SignupDate'])
transactions_df['TransactionDate'] = pd.to_datetime(transactions_df['TransactionDate'])

# Create customer features
customer_features = customers_df.copy()
customer_features['DaysSinceSignup'] = (pd.Timestamp.now() - customer_features['SignupDate']).dt.days

# Create transaction features
transaction_features = transactions_df.groupby('CustomerID').agg({
    'TransactionID': 'count',
    'Quantity': 'sum',
    'TotalValue': 'sum',
    'Price': 'mean'
}).reset_index()

transaction_features.columns = ['CustomerID', 'TransactionCount', 'TotalQuantity', 'TotalSpent', 'AveragePrice']

# Merge customer and transaction features
customer_profiles = customer_features.merge(transaction_features, on='CustomerID', how='left')

# Fill NaN values for customers with no transactions
customer_profiles.fillna(0, inplace=True)

# Encode categorical variables
customer_profiles = pd.get_dummies(customer_profiles, columns=['Region'])

# Select features for clustering
clustering_features = ['DaysSinceSignup', 'TransactionCount', 'TotalQuantity', 'TotalSpent', 'AveragePrice',
                       'Region_Asia', 'Region_Europe', 'Region_North America', 'Region_South America']

# Normalize the features
scaler = StandardScaler()
normalized_features = scaler.fit_transform(customer_profiles[clustering_features])

# Perform clustering for different numbers of clusters
max_clusters = 10
db_scores = []

for n_clusters in range(2, max_clusters + 1):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(normalized_features)
    db_score = davies_bouldin_score(normalized_features, cluster_labels)
    db_scores.append(db_score)

# Plot DB Index scores
plt.figure(figsize=(10, 6))
plt.plot(range(2, max_clusters + 1), db_scores, marker='o')
plt.title('Davies-Bouldin Index for Different Numbers of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Davies-Bouldin Index')
plt.show()

# Choose the optimal number of clusters (lowest DB Index)
optimal_clusters = db_scores.index(min(db_scores)) + 2
print(f"Optimal number of clusters: {optimal_clusters}")

# Perform final clustering with optimal number of clusters
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
cluster_labels = kmeans.fit_predict(normalized_features)

# Add cluster labels to customer profiles
customer_profiles['Cluster'] = cluster_labels

# Visualize clusters (using first three features)
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(normalized_features[:, 0], normalized_features[:, 1], normalized_features[:, 2],
                     c=cluster_labels, cmap='viridis')

ax.set_xlabel(clustering_features[0])
ax.set_ylabel(clustering_features[1])
ax.set_zlabel(clustering_features[2])

plt.title(f'Customer Segmentation ({optimal_clusters} clusters)')
plt.colorbar(scatter)
plt.show()

# Calculate clustering metrics
db_index = davies_bouldin_score(normalized_features, cluster_labels)
print(f"Davies-Bouldin Index: {db_index}")

# Analyze cluster characteristics
cluster_summary = customer_profiles.groupby('Cluster').agg({
    'DaysSinceSignup': 'mean',
    'TransactionCount': 'mean',
    'TotalQuantity': 'mean',
    'TotalSpent': 'mean',
    'AveragePrice': 'mean'
}).round(2)

print("\nCluster Summary:")
print(cluster_summary)

# Save clustering results
customer_profiles.to_csv('FirstName_LastName_Clustering_Results.csv', index=False)
print("\nClustering results saved to FirstName_LastName_Clustering_Results.csv")
